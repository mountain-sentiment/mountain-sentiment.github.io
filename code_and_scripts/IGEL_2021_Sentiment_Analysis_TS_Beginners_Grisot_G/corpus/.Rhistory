library(tidytext)
library(tidyverse)
library(wordcloud)
library(textdata)
library(readr)
library(data.table)
library(purrr)
library(syuzhet)
library(tm)
options(stringsAsFactors = F, # do not convert upon loading
scipen = 999, # do not convert numbers to e-values
max.print = 200) # stop printing after 200 values
theme_set(theme_light()) # set default ggplot theme to light
fs = 12 # default plot font size
setwd("corpus")
austen_corpus <- austen_files %>%
set_names(.) %>%
map_df(read_table, .id = "FileName")
setwd("C:/Users/grig/Dropbox/IGEL Training School 2021 on Sentiment Analysis/Day1_practice_GG")
setwd("corpus")
austen_files <- list.files(pattern = ".*.txt") # this tells R to look only for txt files in the working directory
austen_corpus <- austen_files %>%
set_names(.) %>%
map_df(read_table, .id = "FileName")
austen_corpus <- austen_corpus %>%
separate(FileName, into = c("author", "title", "year"), sep = "_", remove = T) %>%
mutate(year = str_remove(str_trim(year, side = "both"), ".txt"))
austen_corpus <- austen_corpus %>%
group_by(title) %>%
mutate(sentence_id = seq_along(text)) %>%
ungroup() %>%
select(author,
title,
year,
sentence_id,
text) %>%
unnest_tokens(word, text, to_lower = T) # to_lower allow us to convert words to lower case
austen_corpus <- austen_corpus %>%
group_by(title, sentence_id) %>%
mutate(word_id = seq_along(word)) %>%
ungroup()sz
austen_corpus <- austen_corpus %>%
group_by(title, sentence_id) %>%
mutate(word_id = seq_along(word)) %>%
ungroup()
head(austen_corpus)
austen_corpus %>%
group_by(title, word) %>%
anti_join(stop_words, by = "word") %>% # delete stopwords
count() %>% # summarize count per word per title
arrange(desc(n)) %>% # highest freq on top
group_by(title) %>% #
mutate(top = seq_along(word)) %>% # identify rank within group
filter(top <= 15) %>% # retain top 15 frequent words
# create barplot
ggplot(aes(x = -top, fill = title)) +
geom_bar(aes(y = n), stat = 'identity', col = 'black') +
# make sure words are printed either in or next to bar
geom_text(aes(y = ifelse(n > max(n) / 2, max(n) / 50, n + max(n) / 50),
label = word), size = fs/3, hjust = "left") +
theme(legend.position = 'none', # get rid of legend
text = element_text(size = fs), # determine fs
axis.text.x = element_text(angle = 45, hjust = 1, size = fs/1.5), # rotate x text
axis.ticks.y = element_blank(), # remove y ticks
axis.text.y = element_blank()) + # remove y text
labs(y = "Word count", x = "", # add labels
title = "Austen: Most frequent words throughout the novels") +
facet_grid(. ~ title) + # separate plot for each title
coord_flip() # flip axes
test <- austen_corpus %>%
ungroup() %>%
group_split(title)
test2 = list()
for (i in 1:length(test)) {
avg_ch_lenght <- nrow(test[[i]])/15
r  <- rep(1:ceiling(nrow(test[[i]])/avg_ch_lenght),each=avg_ch_lenght)[1:nrow(test[[i]])]
test2[[i]] <- split(test[[i]],r)
}
for (i in 1:length(test2)) {
for (j in 1:length(test2[[i]])) {
test2[[i]][[j]]$chapter <- paste0(j)
}
}
test = list()
for (i in 1:length(test2)) {
test[[i]] <- data.table::rbindlist(test2[[i]])
}
austen_corpus <- data.table(rbindlist(test))
remove(test, test2)
austen_corpus
head(austen_corpus)
austen_SA <- bind_rows(
# 1 AFINN
austen_corpus %>%
left_join(get_sentiments("afinn"), by = "word")  %>%
filter(value != 0) %>% # delete neutral words
mutate(sentiment = ifelse(value < 0, 'negative', 'positive')) %>% # identify sentiment
mutate(value = sqrt(value ^ 2)) %>% # all values to positive
group_by(title, sentiment) %>%
mutate(dictionary = 'afinn'), # create dictionary identifier
# 2 BING
austen_corpus %>%
left_join(get_sentiments("bing"), by = "word") %>%
group_by(title, sentiment) %>%
mutate(dictionary = 'bing'), # create dictionary identifier
# 3 NRC
austen_corpus %>%
left_join(get_sentiments("nrc"), by = "word") %>%
group_by(title, sentiment) %>%
mutate(dictionary = 'nrc') %>% # create dictionary identifier
ungroup()
)
austen_SA %>% head()
wordcloud::wordcloud(words = austen_SA$word, min.freq = 3, max.words = 100)
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
wordcloud::wordcloud(words = word, min.freq = 3, max.words = 100)
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
with(wordcloud(words = word, min.freq = 3, max.words = 100))
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
with(wordcloud(words = word, min.freq = 3, max.words = 500))
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
with(wordcloud(words = word, min.freq = 3, max.words = 40))
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
with(wordcloud(words = word, min.freq = 3, max.words = 70))
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
with(wordcloud(words = word, min.freq = 3, max.words = 70))
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
with(wordcloud(words = word, min.freq = 3, max.words = 50))
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
with(wordcloud(words = word, min.freq = 3, max.words = 50, colors = sentiment))
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
with(wordcloud(words = word, min.freq = 3, max.words = 50, colors = as.factor(dictionary)))
install.packages("ggwordcloud")
library(ggwordcloud)
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
ggwordcloud()
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
ggwordcloud(words = word)
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
ggplot(aes(word), fill = dictionary) %>%
ggwordcloud()
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
ggplot(aes(label = word), fill = dictionary) %>%
ggwordcloud()
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
ggplot(aes(label = word), fill = dictionary) %>%
geom_text_wordcloud()
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
ggplot(aes(label = word), fill = dictionary) +
geom_text_wordcloud()
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
ggplot(aes(label = word), fill = dictionary) +
geom_text_wordcloud() +
scale_size_area(max_size = 40)
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
ggplot(aes(label = word), fill = dictionary) +
geom_text_wordcloud() +
scale_size_area(max_size = 40)
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
group_by(word) %>%
count() %>% # summarize count per word
mutate(log_n = sqrt(n)) %>% # take root to decrease outlier impact
with(wordcloud(word, log_n, max.words = 100))
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
group_by(word) %>%
count() %>% # summarize count per word
mutate(log_n = sqrt(n)) %>% # take root to decrease outlier impact
with(wordcloud(word, log_n, max.words = 100))
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
group_by(word) %>%
count() %>% # summarize count per word
mutate(log_n = sqrt(n)) %>% # take root to decrease outlier impact
with(wordcloud(word, log_n, max.words = 100))
View(austen_SA)
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
group_by(word)
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
filter(dictionary == "afinn") %>%
anti_join(stop_words, by = "word") %>% # delete stopwords
group_by(word)
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
filter(dictionary == "afinn") %>%
anti_join(stop_words, by = "word") %>% # delete stopwords
group_by(word) %>%
count() %>% # summarize count per word
mutate(log_n = sqrt(n)) %>% # take root to decrease outlier impact
with(wordcloud(word, log_n, max.words = 100))
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
filter(dictionary == "afinn") %>%
anti_join(stop_words, by = "word") %>% # delete stopwords
group_by(word) %>%
count() %>% # summarize count per word
mutate(log_n = sqrt(n)) %>% # take root to decrease outlier impact
with(wordcloud(word, log_n, max.words = 30))
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
group_by(word) %>%
count() %>% # summarize count per word
mutate(log_n = sqrt(n)) %>% # take root to decrease outlier impact
with(wordcloud(word, log_n, max.words = 100, ))
library(tidytext)
library(tidyverse)
library(wordcloud)
library(textdata)
library(readr)
library(data.table)
library(purrr)
library(syuzhet)
library(tm)
# set options --------------------
# we can start setting up a few options for our project
options(stringsAsFactors = F, # do not convert upon loading
scipen = 999, # do not convert numbers to e-values
max.print = 200) # stop printing after 200 values
theme_set(theme_light()) # set default ggplot theme to light
fs = 12 # default plot font size
# files import -----------------
# in this case, the files are consistently saved as author_title_year.txt, where we use one word only for the author and for the title, and the format YYYY for the year of first publication.
# It is important to be consistent! It can make your life much easier when you deal with many texts.
# now, we need to instruct R to look into the directory where we have stored our txt files.
# you can do so by copying and pasting the directory path to your corpus, or use the navigation panel to get inside the folder where the txt files are, then click on the little arrow near the "More" icon, and select "Set As Working Directory".
setwd("corpus")
austen_files <- list.files(pattern = ".*.txt") # this tells R to look only for txt files in the working directory
# with the next chink we make sure we keep the filename and use it as a variable, which then can be useful for preserving information we stored in the filename
austen_corpus <- austen_files %>%
set_names(.) %>%
map_df(read_table, .id = "FileName")
# as mentioned above, we can then split the filename into the relevant "pieces" of information we want.
# we tell R to split the filename into author, title and year, looking at "_" symbols as separator. we can achieve that with the "separate" function.
# because the file extension is not separated from the rest of the filename with a "_" symbol, we need to tell R to remove the ".txt" part, and we can do so with "str_remove". We also make sure there are no white spaces on wither side of the year with "str_trim"
austen_corpus <- austen_corpus %>%
separate(FileName, into = c("author", "title", "year"), sep = "_", remove = T) %>%
mutate(year = str_remove(str_trim(year, side = "both"), ".txt"))
# and we can now add an id per sentence and then tokenize
austen_corpus <- austen_corpus %>%
group_by(title) %>%
mutate(sentence_id = seq_along(text)) %>%
ungroup() %>%
select(author,
title,
year,
sentence_id,
text) %>%
unnest_tokens(word, text, to_lower = T) # to_lower allow us to convert words to lower case
# we can also create a word identification number per title per sentence
austen_corpus <- austen_corpus %>%
group_by(title, sentence_id) %>%
mutate(word_id = seq_along(word)) %>%
ungroup()
# let's have a look at out dataset now
head(austen_corpus)
# plot word frequency ----------------------
# now we can have a first look at our corpus and see which words are most frequent in the novels
austen_corpus %>%
group_by(title, word) %>%
anti_join(stop_words, by = "word") %>% # delete stopwords
count() %>% # summarize count per word per title
arrange(desc(n)) %>% # highest freq on top
group_by(title) %>% #
mutate(top = seq_along(word)) %>% # identify rank within group
filter(top <= 15) %>% # retain top 15 frequent words
# create barplot
ggplot(aes(x = -top, fill = title)) +
geom_bar(aes(y = n), stat = 'identity', col = 'black') +
# make sure words are printed either in or next to bar
geom_text(aes(y = ifelse(n > max(n) / 2, max(n) / 50, n + max(n) / 50),
label = word), size = fs/3, hjust = "left") +
theme(legend.position = 'none', # get rid of legend
text = element_text(size = fs), # determine fs
axis.text.x = element_text(angle = 45, hjust = 1, size = fs/1.5), # rotate x text
axis.ticks.y = element_blank(), # remove y ticks
axis.text.y = element_blank()) + # remove y text
labs(y = "Word count", x = "", # add labels
title = "Austen: Most frequent words throughout the novels") +
facet_grid(. ~ title) + # separate plot for each title
coord_flip() # flip axes
# because we do not have chapters data in our dataset, we can arbitrarily assign "fake chapters" to the novels, to see the evolution of sentiment throughout. (of course if you have that data already present in your dataset you do not need this)
# for the sake of simplicity, let's split the novels into 15 chapters each.
test <- austen_corpus %>%
ungroup() %>%
group_split(title)
test2 = list()
for (i in 1:length(test)) {
avg_ch_lenght <- nrow(test[[i]])/15
r  <- rep(1:ceiling(nrow(test[[i]])/avg_ch_lenght),each=avg_ch_lenght)[1:nrow(test[[i]])]
test2[[i]] <- split(test[[i]],r)
}
for (i in 1:length(test2)) {
for (j in 1:length(test2[[i]])) {
test2[[i]][[j]]$chapter <- paste0(j)
}
}
test = list()
for (i in 1:length(test2)) {
test[[i]] <- data.table::rbindlist(test2[[i]])
}
austen_corpus <- data.table(rbindlist(test))
remove(test, test2)
# let's have a look again
head(austen_corpus)
# Sentiment lexicons --------------
# so now what about the sentiments?
# first we need to decide which lexicons we can use for sentiments
# in this example, we will use three popular lexicons, namely the AFINN, NRC and BING
# we can thus match these onto our corpus
# we can retrieve sentiment lexicons and apply them to our corpus very easily
# with the the syuzhet package. the function is called get_sentiments
austen_SA <- bind_rows(
# 1 AFINN
austen_corpus %>%
inner_join(get_sentiments("afinn"), by = "word")  %>%
filter(value != 0) %>% # delete neutral words
mutate(sentiment = ifelse(value < 0, 'negative', 'positive')) %>% # identify sentiment
mutate(value = sqrt(value ^ 2)) %>% # all values to positive
group_by(title, sentiment) %>%
mutate(dictionary = 'afinn'), # create dictionary identifier
# 2 BING
austen_corpus %>%
inner_join(get_sentiments("bing"), by = "word") %>%
group_by(title, sentiment) %>%
mutate(dictionary = 'bing'), # create dictionary identifier
# 3 NRC
austen_corpus %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
group_by(title, sentiment) %>%
mutate(dictionary = 'nrc') %>% # create dictionary identifier
ungroup()
)
austen_SA %>% head()
wordcloud::wordcloud(words = austen_SA$word, min.freq = 3)
austen_SA %>%
filter(!is.na(sentiment)) %>% # let's only consider words with a sentiment value
anti_join(stop_words, by = "word") %>% # delete stopwords
group_by(word) %>%
count() %>% # summarize count per word
mutate(log_n = sqrt(n)) %>% # take root to decrease outlier impact
with(wordcloud(word, log_n, max.words = 100, ))
austen_SA %>%
group_by(word) %>%
count() %>% # summarize count per word
mutate(log_n = sqrt(n)) %>% # take root to decrease outlier impact
with(wordcloud(word, log_n, max.words = 100))
gs()
gs()
austen_SA %>%
group_by(word) %>%
count() %>% # summarize count per word
mutate(log_n = sqrt(n)) %>% # take root to decrease outlier impact
with(wordcloud(word, log_n, max.words = 100))
austen_SA %>%
group_by(word) %>%
count() %>% # summarize count per word
mutate(log_n = sqrt(n)) %>% # take root to decrease outlier impact
with(wordcloud(word, log_n, max.words = 100))
wordcloud_plot <- austen_SA %>%
group_by(word) %>%
count() %>% # summarize count per word
mutate(log_n = sqrt(n)) %>% # take root to decrease outlier impact
with(wordcloud(word, log_n, max.words = 100))
par("mar")
